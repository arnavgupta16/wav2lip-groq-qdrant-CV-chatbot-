{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBPEdzXqEYXM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msrKIg6xNXYZ",
        "outputId": "9f4fc57a-8966-42b4-c160-75c414d3539a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.22.0 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.22.0 which is incompatible.\n",
            "pywavelets 1.6.0 requires numpy<3,>=1.22.4, but you have numpy 1.22.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.22.0 which is incompatible.\n",
            "statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.22.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q TTS langchain ollama torch langchain_community  sentence_transformers spacy langchain_qdrant numpy spacy gradio pypdf2 langchain-huggingface langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOF7Vi__QEj2",
        "outputId": "4b1831a4-3eee-4dcf-eb44-4ed207b15f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Wav2Lip' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Rudrabha/Wav2Lip.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r /content/Wav2Lip/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXZfmW3HjlAs",
        "outputId": "d641600b-f265-42a6-da2e-fab4586b055d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting librosa==0.7.0 (from -r /content/Wav2Lip/requirements.txt (line 1))\n",
            "  Using cached librosa-0.7.0.tar.gz (1.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.17.1 (from -r /content/Wav2Lip/requirements.txt (line 2))\n",
            "  Using cached numpy-1.17.1.zip (6.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-contrib-python>=4.2.0.34 in /usr/local/lib/python3.10/dist-packages (from -r /content/Wav2Lip/requirements.txt (line 3)) (4.8.0.76)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.1.0.25 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64, 4.6.0.66, 4.7.0.68, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for opencv-python==4.1.0.25\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6skeCZK8j-Xb",
        "outputId": "acd578ac-698b-470f-dec0-7364392e9c9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.22.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiYd_7dTk9qf",
        "outputId": "7dbc4e83-43c8-4da0-e333-857bdb654841"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.0)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.0\n",
            "    Uninstalling numpy-1.22.0:\n",
            "      Successfully uninstalled numpy-1.22.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy==1.22.0; python_version <= \"3.10\", but you have numpy 2.0.0 which is incompatible.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n",
            "gruut 2.2.3 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n",
            "langchain 0.2.5 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "langchain-community 0.2.5 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.0.0 which is incompatible.\n",
            "numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n",
            "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"\""
      ],
      "metadata": {
        "id": "bmllhNJAwAFt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NrmzpJPiTvcU",
        "outputId": "309cef75-037e-4589-c214-2bca480f6ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using numpy version 1.22.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "<ipython-input-25-21bc01d9673f>:68: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
            "  client.recreate_collection(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://4489a0d82ff143df23.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4489a0d82ff143df23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 532, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1928, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1514, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 832, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-25-21bc01d9673f>\", line 244, in chatbot\n",
            "    response = respond_to_query(query, retriever)\n",
            "  File \"<ipython-input-25-21bc01d9673f>\", line 178, in respond_to_query\n",
            "    if isinstance(response, AIMessage):\n",
            "NameError: name 'AIMessage' is not defined\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "import spacy\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import numpy as np\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "  # replace with actual method to fetch the key\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\", groq_api_key=groq_api_key)\n",
        "\n",
        "# Custom VectorStoreRetriever class\n",
        "class VectorStoreRetriever:\n",
        "    def __init__(self, client, collection_name, embedding_function):\n",
        "        self.client = client\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_function = embedding_function\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        embedding = self.embedding_function.embed_documents([query])[0]\n",
        "        search_result = self.client.search(\n",
        "            collection_name=self.collection_name,\n",
        "            query_vector=embedding,\n",
        "            limit=5  # Adjust as needed\n",
        "        )\n",
        "        return [hit.payload['content'] for hit in search_result]\n",
        "\n",
        "\n",
        "try:\n",
        "    # Ensure compatibility with the latest numpy\n",
        "    import numpy as np\n",
        "    np_version = np.__version__\n",
        "    print(f\"Using numpy version {np_version}\")\n",
        "\n",
        "    # Load spaCy model for NLP tasks\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=20,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False\n",
        "    )\n",
        "\n",
        "    # Initialize embeddings model\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en\")\n",
        "\n",
        "    # Determine the dimension of the embeddings\n",
        "    sample_text = \"This is a sample text to determine the embedding dimension.\"\n",
        "    sample_embedding = embedding_function.embed_documents([sample_text])[0]\n",
        "    embedding_dimension = len(sample_embedding)\n",
        "\n",
        "    # Initialize Qdrant client with in-memory storage\n",
        "    client = QdrantClient(\":memory:\")\n",
        "    collection_name = \"cv_sections\"\n",
        "\n",
        "    # Check and create collection if it doesn't exist\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.recreate_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=models.Distance.COSINE,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # Function to extract and structure CV information\n",
        "    def extract_cv_info(cv_text):\n",
        "        doc = nlp(cv_text)\n",
        "\n",
        "        sections = {\n",
        "            \"skills\": [],\n",
        "            \"experience\": [],\n",
        "            \"education\": []\n",
        "        }\n",
        "\n",
        "        current_section = None\n",
        "        for token in doc:\n",
        "            if token.text.lower() in [\"skills\", \"experience\", \"education\"]:\n",
        "                current_section = token.text.lower()\n",
        "            elif current_section:\n",
        "                sections[current_section].append(token.text)\n",
        "\n",
        "        for key in sections:\n",
        "            sections[key] = \" \".join(sections[key])\n",
        "\n",
        "        return sections\n",
        "\n",
        "    # Function to chunk text\n",
        "    def chunk_text(text):\n",
        "        return text_splitter.split_text(text)\n",
        "\n",
        "    # Function to generate embeddings\n",
        "    def generate_embeddings(text_chunks):\n",
        "        return embedding_function.embed_documents(text_chunks)\n",
        "\n",
        "    # Function to index data in Qdrant\n",
        "    def index_data_in_qdrant(data, collection_name):\n",
        "        points = []\n",
        "        for section, content in data.items():\n",
        "            chunks = chunk_text(content)\n",
        "            embeddings = generate_embeddings(chunks)\n",
        "            for i, embedding in enumerate(embeddings):\n",
        "                points.append(models.PointStruct(\n",
        "                    id=str(uuid.uuid4()),  # Generate a valid UUID for each point\n",
        "                    vector=embedding,\n",
        "                    payload={\"section\": section, \"content\": chunks[i]}\n",
        "                ))\n",
        "\n",
        "        # Upload points to Qdrant\n",
        "        client.upsert(\n",
        "            collection_name=collection_name,\n",
        "            points=points\n",
        "        )\n",
        "\n",
        "    # Function to apply payload filtering and retrieve data\n",
        "    def retrieve_data_with_filter(collection_name, filter_conditions):\n",
        "        query_filter = models.Filter(\n",
        "            must=[models.FieldCondition(\n",
        "                key=key,\n",
        "                match=models.MatchValue(value=value)\n",
        "            ) for key, value in filter_conditions.items()]\n",
        "        )\n",
        "\n",
        "        # Create a dummy vector for filter-only search and ensure it's a float\n",
        "        query_vector = np.zeros(embedding_dimension, dtype=np.float32)\n",
        "\n",
        "        search_result = client.search(\n",
        "            collection_name=collection_name,\n",
        "            query_vector=query_vector.tolist(),  # Convert to list for Qdrant\n",
        "            limit=10,\n",
        "            query_filter=query_filter\n",
        "        )\n",
        "\n",
        "        return [hit.payload for hit in search_result]\n",
        "\n",
        "    # Function to extract text from PDF\n",
        "    def extract_text_from_pdf(pdf_file):\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(pdf_file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "    # Function to convert CV to structured docs and store in Qdrant\n",
        "    def convert_cv_to_structured_docs(cv_text, collection_name, filter_conditions):\n",
        "        structured_data = extract_cv_info(cv_text)\n",
        "        index_data_in_qdrant(structured_data, collection_name)\n",
        "        return retrieve_data_with_filter(collection_name, filter_conditions)\n",
        "\n",
        "    # Define the ChatPromptTemplate for user interaction\n",
        "    template = \"\"\"Answer the following question from the context\n",
        "    context = {context}\n",
        "    question = {question}\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "    # Function to get context for query\n",
        "    def get_context(query, retriever):\n",
        "        return retriever.retrieve(query)\n",
        "\n",
        "    # Function to respond to query\n",
        " # Function to respond to query\n",
        "    def respond_to_query(query, retriever):\n",
        "      context = get_context(query, retriever)\n",
        "      response = llm.invoke(prompt.format(question=query, context=\" \".join(context)))\n",
        "\n",
        "    # Check if response is an instance of AIMessage\n",
        "      if isinstance(response, AIMessage):\n",
        "        # Access the content from AIMessage\n",
        "          if isinstance(response.content, str):\n",
        "              return response.content.strip()\n",
        "          elif isinstance(response.content, list):\n",
        "            # Handle list of contents, if needed\n",
        "              return response.content[0].strip()  # Assuming first item in list is relevant\n",
        "          else:\n",
        "              raise ValueError(\"Unexpected content format in AIMessage\")\n",
        "      else:\n",
        "          raise ValueError(\"Unexpected response format from LLM\")\n",
        "\n",
        "\n",
        "    # Function to convert text to speech\n",
        "    def text_to_speech(text):\n",
        "        command = [\n",
        "            'tts',\n",
        "            '--text', text,\n",
        "            '--model_name', 'tts_models/multilingual/multi-dataset/xtts_v2',\n",
        "            '--vocoder_name', 'vocoder_models/universal/libri-tts/wavegrad',\n",
        "            '--out_path', 'output.wav',\n",
        "            '--speaker_idx', 'Brenda Stern',\n",
        "            '--language_idx', 'en'\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            print(\"TTS generation successful, output saved to 'output.wav'\")\n",
        "            with open(\"output.wav\", \"rb\") as audio_file:\n",
        "                return audio_file.read()\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"An error occurred: {e.stderr.decode()}\")\n",
        "            return None\n",
        "\n",
        "    # Function to create avatar video using Wav2Lip\n",
        "    def create_avatar_video(audio_file_path):\n",
        "        wav2lip_dir = '/content/Wav2Lip'\n",
        "        os.chdir(wav2lip_dir)\n",
        "\n",
        "        command = [\n",
        "            'python', 'inference.py',\n",
        "            '--checkpoint_path', 'checkpoints/wav2lip.pth',\n",
        "            '--face', 'face.mp4',\n",
        "            '--audio', audio_file_path\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            print(\"Avatar video generation successful\")\n",
        "            video_path = 'results/result_voice.mp4'\n",
        "            with open(video_path, \"rb\") as video_file:\n",
        "                return video_file.read()\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"An error occurred: {e.stderr.decode()}\")\n",
        "            return None\n",
        "\n",
        "    # Initialize the retriever\n",
        "    retriever = VectorStoreRetriever(client=client, collection_name=collection_name, embedding_function=embedding_function)\n",
        "\n",
        "    # Gradio interface\n",
        "    def chatbot(cv_file, query):\n",
        "        cv_text = extract_text_from_pdf(cv_file)\n",
        "\n",
        "        filter_conditions = {}  # Define appropriate filter conditions based on requirements\n",
        "        structured_docs = convert_cv_to_structured_docs(cv_text, collection_name, filter_conditions)\n",
        "\n",
        "        response = respond_to_query(query, retriever)\n",
        "\n",
        "        audio_data = text_to_speech(response)\n",
        "\n",
        "        video_file_path = create_avatar_video(\"output.wav\")\n",
        "\n",
        "        return response, audio_data, video_file_path\n",
        "\n",
        "    # Gradio app setup\n",
        "    inputs = [\n",
        "        gr.File(label=\"Upload CV (PDF)\"),\n",
        "        gr.Textbox(label=\"Enter your query\")\n",
        "    ]\n",
        "\n",
        "    outputs = [\n",
        "        gr.Textbox(label=\"Response\"),\n",
        "        gr.Audio(label=\"Audio Response\"),\n",
        "        gr.Video(label=\"Avatar Video Response\")\n",
        "    ]\n",
        "\n",
        "    gr.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"CV Chatbot\").launch(debug=True)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVKXCf_JFn8q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}